{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55db827c-b40d-4788-9834-22ac2204ec04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-vectorsearch\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e83e3d7f-bb97-4b2b-bbfa-5030fee9b0e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Define the catalog name for each user \n",
    "workspace_url = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\n",
    "digits = ''.join(filter(str.isdigit, workspace_url))\n",
    "catalog = 'catalog_' + digits\n",
    "\n",
    "#Define the schema name for the workshop\n",
    "schema=\"agent_workshop\"\n",
    "\n",
    "#define the full schema name\n",
    "full_schema_name = f\"{catalog}.{schema}\"\n",
    "\n",
    "#Define necessary tables\n",
    "guidance_table = f\"{catalog}.{schema}.guidance_unstructure\"\n",
    "\n",
    "#Define the vector search index details\n",
    "vs_index_name = f\"{catalog}.{schema}.guidance_gold_index\"\n",
    "\n",
    "#define the vector search endpoint name (this will be holding the vector index)\n",
    "vs_endpoint_name = \"agent-workshop-vs-endpoint\"\n",
    "\n",
    "#Input table with unstructured data to perform vector index upon\n",
    "vs_input_table = guidance_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53d8e5fc-516a-4e99-9692-d91f404c9076",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Ingest Unstructured Guidance Data\n",
    "\n",
    "This step reads text files containing guidance on portfolio, fraud, and market volatility from the `00-input_data`. Each file is loaded as a row with its content and file path, and a unique `file_id` is added for indexing or lookup purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7156edb3-8c81-416b-bb51-5fede02a2acd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -- 05 - Store unstructured guidance data on portfolio, fraud, and market volatility \n",
    "\n",
    "import os\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "unstructured = []\n",
    "for root, dirs, files in os.walk('00-input_data'):\n",
    "    for file in files:\n",
    "        with open(os.path.join(root, file), 'r') as f:\n",
    "            file_name = os.path.join(root, file)\n",
    "            file_content = str(f.read())\n",
    "            unstructured.append(Row(file_name=file_name, file_content=file_content))\n",
    "\n",
    "# Create DataFrame and add file_id\n",
    "df = spark.createDataFrame(unstructured).withColumn(\"file_id\", monotonically_increasing_id())\n",
    "\n",
    "# Store the data into Unity Catalog\n",
    "df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", 'true').saveAsTable(guidance_table)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "968c49f9-af32-4296-96db-738ed96e66a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Generate Vector Search Index\n",
    "Creating a vector search endpoint (if it doesnâ€™t already exist, you can check in **Compute** then **Vector Search**)\n",
    "\n",
    "Creating a **[Databricks Vector Search](https://docs.databricks.com/aws/en/generative-ai/vector-search)** index on top of the unstructured guidance data (e.g. portfolio, fraud, market volatility) This index allows us to do fast similarity searches. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0da150c-7635-4e1b-9c0b-a9f70f3da466",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "\n",
    "# The following line automatically generates a PAT Token for authentication\n",
    "client = VectorSearchClient()\n",
    "try:\n",
    "    client.create_endpoint(\n",
    "        name=vs_endpoint_name,\n",
    "        endpoint_type=\"STANDARD\"\n",
    "    )\n",
    "except:\n",
    "    print('endpoint already exists')\n",
    "\n",
    "# Enable change data feed for the Delta table\n",
    "spark.sql(f\"ALTER TABLE {vs_input_table} SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\")\n",
    "\n",
    "#We are creating a vector search index inside the vector search endpoint with the previous credentials \n",
    "\n",
    "#create_delta_sync_index for databricks managed embeddings\n",
    "index = client.create_delta_sync_index(\n",
    "  endpoint_name=vs_endpoint_name,\n",
    "  source_table_name=vs_input_table,\n",
    "  index_name=vs_index_name,\n",
    "  pipeline_type=\"TRIGGERED\",\n",
    "  primary_key=\"file_name\",\n",
    "  embedding_source_column=\"file_content\",\n",
    "  embedding_model_endpoint_name=\"<databricks embedding model endpoint>\"  #Databricks hosted embedding models \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01b-create-synthetic-data-unstructured",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
