{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73ce743a-e049-44f3-b841-b5709d591410",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U -qqq mlflow langgraph==0.3.4 databricks-langchain databricks-agents uv \n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c01750ec-3d97-4a50-8544-b4ebb220bc1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Generate synthetic data\n",
    "\n",
    "To synthesise evaluations for an agent that uses document retrieval, use the `generate_evals_df` method from the `databricks-agents` Python package. This step automatically creates a set of evaluation questions and answers based on your unstructured guidance documents.\n",
    "\n",
    "- **Input**: A table of documents (`content`, `doc_uri`) and evaluation guidelines\n",
    "- **Output**: A generated set of eval questions stored as a Delta table\n",
    "- **Purpose**: To test and refine retrieval performance using realistic, autogenerated examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99fda659-680c-4e5d-9117-31c993b063b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Define the catalog name for each user \n",
    "user_email = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get() \n",
    "email = str(user_email).split('@')\n",
    "catalog = email[0].replace('.','')\n",
    "\n",
    "schema = \"agent_workshop\"\n",
    "\n",
    "num_evals = 30 \n",
    "model_version = 1\n",
    "\n",
    "table_name = f\"{catalog}.{schema}.guidance_unstructure\"\n",
    "eval_table = f\"{catalog}.{schema}.finOps_synthetic_evals\"\n",
    "model_name = f\"models:/{catalog}.{schema}.financial_assistance_agent/{model_version}\"\n",
    "\n",
    "# FinOps portfolio analyst persona , guideline for synthetic data generation \n",
    "guidelines = f\"\"\"\n",
    "# Task Description\n",
    "The agent is a financial portfolio advisor and a fraud assistant agent. The agents task is to provide guidance on difference scenarios of rebalancing , portfolio assessment , fraud analysis guidance , procedures of next steps if fraud happens etc.s\n",
    "\n",
    "# User personas\n",
    "- A customer who wants to assess and understand their portfolio and seek guidance on management.\n",
    "- A customer who wants to understand the steps to take if fraud happens.\n",
    "- A customer who wants to get guidance on portfolio rebalancing.\n",
    "\n",
    "# Example questions\n",
    "- What is the standard procedure for rebalancing my portfolio and what should i consider?\n",
    "- How much time does it take for a fraud incident to resolve ?\n",
    "\n",
    "# Additional Guidelines\n",
    "- Questions should be succinct, and human-like\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "339882d2-e9c5-4fd1-adc3-a6ae9abbf849",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use the synthetic eval generation API to get some evals\n",
    "from databricks.agents.evals import generate_evals_df\n",
    "\n",
    "parsed_docs_df = spark.table(table_name).withColumnRenamed('file_name', 'doc_uri').withColumnRenamed('file_content', 'content').toPandas()\n",
    "\n",
    "evals = generate_evals_df(\n",
    "    docs=parsed_docs_df, # Pass your docs. Pandas/Spark Dataframes with columns `content STRING, doc_uri STRING` are suitable.\n",
    "    num_evals=num_evals, # How many synthetic evaluations to generate\n",
    "    question_guidelines=guidelines\n",
    ")\n",
    "display(evals)\n",
    "\n",
    "#store the synthetic evaluation set into a delta table\n",
    "spark.createDataFrame(evals).write.format(\"delta\").mode(\"overwrite\").saveAsTable(eval_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9a12e74-d74f-40b5-b389-8487603e7474",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Offline evaluation via Mosaic AI eval\n",
    "\n",
    "- Built-in LLM Judges\n",
    "- Global Guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2e0e486-e8a2-48f9-841a-22688600e489",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow \n",
    "itern = 1\n",
    "with mlflow.start_run(run_name=f\"finOps_assistant_offline_eval_{itern}\") as run: \n",
    "  # Run evaluation\n",
    "  eval_results = mlflow.evaluate(\n",
    "    data=evals,  # The latest evaluation dataset\n",
    "    model=model_name,  \n",
    "    model_type=\"databricks-agent\",  \n",
    "    evaluator_config={\n",
    "          \"databricks-agent\": {\n",
    "              \"metrics\": ['relevance_to_query',\n",
    "                          'safety', \n",
    "                          'groundedness',\n",
    "                          'correctness', \n",
    "                          'chunk_relevance', \n",
    "                          'guideline_adherence'\n",
    "                          ],\n",
    "            \"global_guidelines\": [\"The response must be on the domain of Financial Assistant or Fraud analysis and suggest  financial guidance\", \n",
    "                                  \"The response should not contain inconsistant answer to the question\",\n",
    "                                  \"The response should not generate hypothetical answer\", \n",
    "                                  \"The response should not contain harmful or discriminatory content\"]\n",
    "          }\n",
    "      }\n",
    "    )\n",
    "  \n",
    "iter +=1"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "04-validate-agent-performance",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
